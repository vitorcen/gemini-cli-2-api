# API æ¥å£å®ç°åˆ†æä¸æµ‹è¯•æ–¹æ¡ˆ

## ã€Tasteã€‘ğŸŸ¡Passable

å®ç°åŸºæœ¬å¯ç”¨ï¼Œä½†è·ç¦»ç”Ÿäº§çº§åˆ«å®Œæ•´ API è¿˜æœ‰æ˜æ˜¾å·®è·ã€‚

---

## ã€Fatal Issuesã€‘è‡´å‘½ç¼ºé™·

### 1. **OpenAI æ¥å£ (openaiProxy.ts)**

#### ğŸ”´ ç¼ºå¤± Tools/Function Calling
- **æ ‡å‡†è¦æ±‚**: `tools` å‚æ•° + `tool_choice` + `tool_calls` å“åº”
- **å½“å‰å®ç°**: å®Œå…¨ç¼ºå¤±
- **å½±å“**: æ— æ³•æ”¯æŒä»»ä½•éœ€è¦å·¥å…·è°ƒç”¨çš„åœºæ™¯ï¼ˆå¦‚ LangChain, AutoGPT ç­‰ï¼‰

#### ğŸ”´ æ¶ˆæ¯å¤„ç†è¿‡äºç®€åŒ–
```typescript
// ç°åœ¨ï¼šç›´æ¥æ‹¼æ¥æ‰€æœ‰æ¶ˆæ¯ä¸ºå•ä¸ªæ–‡æœ¬
const mergedUserText = (body.messages || [])
  .filter((m) => m && (m.role === 'system' || m.role === 'user'))
  .map((m) => (m.role === 'system' ? `(system) ${m.content}` : m.content))
  .join('\n\n');
```
- **é—®é¢˜**: ä¸¢å¤± assistant å†å²æ¶ˆæ¯ï¼Œå¤šè½®å¯¹è¯å®Œå…¨ä¸å¯ç”¨
- **æ ‡å‡†**: åº”ä¿æŒå®Œæ•´çš„ messages æ•°ç»„ï¼ŒåŒ…æ‹¬ assistant å›å¤

#### ğŸ”´ ç¼ºå¤±å…³é”®å‚æ•°
- `n` (ç”Ÿæˆå¤šä¸ªå›å¤)
- `stop` (åœæ­¢åºåˆ—)
- `presence_penalty` / `frequency_penalty`
- `logit_bias`
- `user` (ç”¨æˆ·æ ‡è¯†)

---

### 2. **Claude æ¥å£ (claudeProxy.ts)**

#### ğŸŸ¡ å·¥å…·è°ƒç”¨å®ç°ä¸å®Œæ•´
```typescript
// å½“å‰æœ‰ä¸¤ä¸ªè·¯å¾„ï¼š
// 1. ç†æƒ³è·¯å¾„ï¼šç»“æ„åŒ– functionCall
// 2. é™çº§è·¯å¾„ï¼šè§£æ [tool_code] æ ‡è®°
```
- **é—®é¢˜**: Gemini çš„ functionCall æ ¼å¼ä¸ Claude tools ä¸å®Œå…¨å¯¹åº”
- **ç¼ºå¤±**:
  - å·¥å…·å®šä¹‰ä¼ é€’åˆ° Gemini (request.tools â†’ Gemini tools)
  - å·¥å…·ç»“æœå›ä¼ å¤„ç† (tool_result content type)
  - `tool_choice` å‚æ•°æ”¯æŒ

#### ğŸ”´ System Prompt å¤„ç†æœ‰è¯¯
```typescript
// å½“å‰å®ç°ï¼šç¡¬ç¼–ç æ³¨å…¥åˆ° contents
const systemInstruction = {
  role: 'user',
  parts: [{ text: `System Instructions:\n${systemContent}\n\nIMPORTANT: Follow these instructions for all responses.` }]
};
```
- **é—®é¢˜**: Gemini 2.0+ æœ‰åŸç”Ÿ systemInstructionï¼Œåº”è¯¥ä½¿ç”¨
- **å‰¯ä½œç”¨**: æµªè´¹ tokenï¼Œä¸”å¯èƒ½è¢«æ¨¡å‹å¿½ç•¥

#### ğŸ”´ æµå¼å“åº”çŠ¶æ€æœºæœ‰ bug
```typescript
let currentBlockType: 'text' | 'tool_use' | null = null;
let currentContentIndex = -1;

// é—®é¢˜ï¼šæ²¡æœ‰æ­£ç¡®å¤„ç†äº¤é”™çš„ text å’Œ tool_use
// å½“ Gemini è¿”å› text + functionCall æ—¶ï¼ŒçŠ¶æ€ç®¡ç†æ··ä¹±
```

---

### 3. **Gemini æ¥å£ (geminiProxy.ts)**

#### ğŸŸ¡ å·¥å…·è°ƒç”¨æ”¯æŒç¼ºå¤±
- è¯·æ±‚ä¸­ `tools.functionDeclarations` æœªä¼ é€’ç»™åº•å±‚
- å“åº”ä¸­ `functionCall` æœªæ˜ å°„åˆ°æ ‡å‡†æ ¼å¼

#### ğŸŸ¢ ç›¸å¯¹å®Œæ•´
- SSE æµå¼æ­£ç¡®
- å‚æ•°æ˜ å°„æ­£ç¡®
- å¢é‡æ–‡æœ¬å¤„ç†æ­£ç¡®

---

## ã€æ ¸å¿ƒæ•°æ®ç»“æ„é—®é¢˜ã€‘

### âŒ ç ´åæ€§è½¬æ¢ï¼šæ¶ˆæ¯å†å²ä¸¢å¤±

```typescript
// OpenAI: æ”¯æŒå®Œæ•´å¯¹è¯å†å²
messages: [
  {role: "user", content: "Hi"},
  {role: "assistant", content: "Hello!"},
  {role: "user", content: "What did I say?"}
]

// å½“å‰å®ç°ï¼šåªä¿ç•™ user/systemï¼Œä¸¢å¼ƒ assistant
// ç»“æœï¼šæ¨¡å‹æ— æ³•çŸ¥é“ä¹‹å‰è¯´è¿‡ä»€ä¹ˆ
```

**æ­£ç¡®åšæ³•**: æ˜ å°„ä¸º Gemini Contents æ•°ç»„
```typescript
contents: [
  {role: "user", parts: [{text: "Hi"}]},
  {role: "model", parts: [{text: "Hello!"}]},
  {role: "user", parts: [{text: "What did I say?"}]}
]
```

---

## ã€ä¸æ ‡å‡†æ¥å£çš„å·®è·å¯¹æ¯”ã€‘

### OpenAI Chat Completions API

| ç‰¹æ€§ | æ ‡å‡†è¦æ±‚ | å½“å‰å®ç° | ä¼˜å…ˆçº§ |
|------|---------|---------|--------|
| åŸºç¡€æ¶ˆæ¯ | âœ… messages æ•°ç»„ | ğŸ”´ ä»…æ‹¼æ¥æ–‡æœ¬ | P0 |
| å·¥å…·è°ƒç”¨ | âœ… tools + tool_calls | ğŸ”´ å®Œå…¨ç¼ºå¤± | P0 |
| æµå¼å“åº” | âœ… SSE delta æ ¼å¼ | âœ… æ­£ç¡® | âœ… |
| å‡½æ•°å‚æ•° | âœ… function_call | ğŸ”´ ç¼ºå¤± | P0 |
| å¤šå›å¤ | âœ… n å‚æ•° | ğŸ”´ ç¼ºå¤± | P1 |
| åœæ­¢åºåˆ— | âœ… stop | ğŸ”´ ç¼ºå¤± | P1 |
| Token æƒ©ç½š | âœ… penalties | ğŸ”´ ç¼ºå¤± | P2 |
| ä½¿ç”¨ç»Ÿè®¡ | âœ… usage | âœ… æ­£ç¡® | âœ… |

### Claude Messages API

| ç‰¹æ€§ | æ ‡å‡†è¦æ±‚ | å½“å‰å®ç° | ä¼˜å…ˆçº§ |
|------|---------|---------|--------|
| æ¶ˆæ¯æ ¼å¼ | âœ… messages | âœ… åŸºæœ¬æ­£ç¡® | âœ… |
| ç³»ç»Ÿæç¤º | âœ… system å­—æ®µ | ğŸŸ¡ ç¡¬ç¼–ç æ³¨å…¥ | P0 |
| å·¥å…·å®šä¹‰ | âœ… tools æ•°ç»„ | ğŸ”´ æœªä¼ é€’ | P0 |
| å·¥å…·ä½¿ç”¨ | âœ… tool_use block | ğŸŸ¡ åŠæˆå“ | P0 |
| å·¥å…·ç»“æœ | âœ… tool_result | ğŸ”´ ç¼ºå¤± | P0 |
| æµå¼äº‹ä»¶ | âœ… 7 ç§äº‹ä»¶ç±»å‹ | ğŸŸ¡ éƒ¨åˆ†å®ç° | P1 |
| æ€ç»´é“¾ | âœ… thinking block | ğŸ”´ ç¼ºå¤± | P2 |

### Gemini API

| ç‰¹æ€§ | æ ‡å‡†è¦æ±‚ | å½“å‰å®ç° | ä¼˜å…ˆçº§ |
|------|---------|---------|--------|
| åŸºç¡€ç”Ÿæˆ | âœ… generateContent | âœ… æ­£ç¡® | âœ… |
| æµå¼ç”Ÿæˆ | âœ… streamGenerateContent | âœ… æ­£ç¡® | âœ… |
| å·¥å…·å£°æ˜ | âœ… functionDeclarations | ğŸ”´ ç¼ºå¤± | P0 |
| å·¥å…·è°ƒç”¨ | âœ… functionCall | ğŸ”´ æœªå¤„ç† | P0 |
| ç³»ç»ŸæŒ‡ä»¤ | âœ… systemInstruction | ğŸ”´ æœªä½¿ç”¨ | P0 |
| æ€ç»´æ¨¡å¼ | âœ… thought parts | ğŸ”´ ç¼ºå¤± | P2 |

---

## ã€å…¼å®¹æ€§é—®é¢˜ã€‘

### å¯¹ claude-code-router çš„å½±å“

```typescript
// router æœŸå¾…æ ‡å‡† Claude APIï¼š
const response = await fetch(`/v1/messages`, {
  method: 'POST',
  body: JSON.stringify({
    model: "claude-3-5-sonnet-20241022",
    max_tokens: 4096,
    messages: [...],
    tools: [...]  // âŒ å½“å‰å®ç°ä¸æ”¯æŒ
  })
});
```

**è‡´å‘½é—®é¢˜**:
1. âŒ å·¥å…·è°ƒç”¨å®Œå…¨ä¸å¯ç”¨ â†’ agent åŠŸèƒ½å¤±æ•ˆ
2. âŒ å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ä¸¢å¤± â†’ æ— æ³•ç»´æŒä¼šè¯
3. âŒ system prompt å¤„ç†ä¸å½“ â†’ è¡Œä¸ºä¸å¯é¢„æµ‹

---

## ã€å•å…ƒæµ‹è¯•éªŒè¯æ–¹æ¡ˆã€‘

### æµ‹è¯•æ–‡ä»¶ç»“æ„
```
packages/a2a-server/src/http/__tests__/
â”œâ”€â”€ openaiProxy.test.ts
â”œâ”€â”€ claudeProxy.test.ts
â”œâ”€â”€ geminiProxy.test.ts
â””â”€â”€ fixtures/
    â”œâ”€â”€ openai-requests.json
    â”œâ”€â”€ claude-requests.json
    â””â”€â”€ gemini-requests.json
```

### 1. OpenAI æ¥å£æµ‹è¯•

```typescript
// packages/a2a-server/src/http/__tests__/openaiProxy.test.ts

describe('OpenAI Proxy', () => {

  describe('Multi-turn Conversations', () => {
    it('should preserve assistant messages in context', async () => {
      const request = {
        model: 'gpt-4',
        messages: [
          { role: 'user', content: 'My name is Alice' },
          { role: 'assistant', content: 'Nice to meet you, Alice!' },
          { role: 'user', content: 'What is my name?' }
        ]
      };

      const response = await POST('/v1/chat/completions', request);

      // åº”è¯¥èƒ½æ­£ç¡®å›ç­” "Alice"ï¼Œå› ä¸ºä¿ç•™äº†å¯¹è¯å†å²
      expect(response.choices[0].message.content).toContain('Alice');
    });
  });

  describe('Function Calling', () => {
    it('should support tools parameter', async () => {
      const request = {
        model: 'gpt-4',
        messages: [{ role: 'user', content: 'What is the weather in SF?' }],
        tools: [{
          type: 'function',
          function: {
            name: 'get_weather',
            description: 'Get weather',
            parameters: {
              type: 'object',
              properties: {
                location: { type: 'string' }
              },
              required: ['location']
            }
          }
        }]
      };

      const response = await POST('/v1/chat/completions', request);

      expect(response.choices[0].finish_reason).toBe('tool_calls');
      expect(response.choices[0].message.tool_calls).toBeDefined();
      expect(response.choices[0].message.tool_calls[0].function.name).toBe('get_weather');
    });
  });

  describe('Streaming with Tools', () => {
    it('should stream tool_calls incrementally', async () => {
      const request = {
        model: 'gpt-4',
        stream: true,
        messages: [{ role: 'user', content: 'Call get_weather for NYC' }],
        tools: [/* ... */]
      };

      const chunks = await streamPOST('/v1/chat/completions', request);

      // éªŒè¯æµå¼å·¥å…·è°ƒç”¨æ ¼å¼
      const toolCallChunks = chunks.filter(c => c.choices[0].delta.tool_calls);
      expect(toolCallChunks.length).toBeGreaterThan(0);
    });
  });
});
```

### 2. Claude æ¥å£æµ‹è¯•

```typescript
// packages/a2a-server/src/http/__tests__/claudeProxy.test.ts

describe('Claude Proxy', () => {

  describe('Tool Use', () => {
    it('should pass tools to Gemini and map responses', async () => {
      const request = {
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1024,
        tools: [{
          name: 'get_weather',
          description: 'Get weather info',
          input_schema: {
            type: 'object',
            properties: {
              location: { type: 'string' }
            },
            required: ['location']
          }
        }],
        messages: [{ role: 'user', content: 'Weather in NYC?' }]
      };

      const response = await POST('/v1/messages', request);

      expect(response.content).toEqual(
        expect.arrayContaining([
          expect.objectContaining({
            type: 'tool_use',
            name: 'get_weather',
            input: expect.objectContaining({
              location: expect.stringContaining('NYC')
            })
          })
        ])
      );
    });

    it('should handle tool_result in messages', async () => {
      const request = {
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1024,
        messages: [
          {
            role: 'user',
            content: 'What is the weather?'
          },
          {
            role: 'assistant',
            content: [
              {
                type: 'tool_use',
                id: 'toolu_123',
                name: 'get_weather',
                input: { location: 'NYC' }
              }
            ]
          },
          {
            role: 'user',
            content: [
              {
                type: 'tool_result',
                tool_use_id: 'toolu_123',
                content: 'Sunny, 72F'
              }
            ]
          }
        ]
      };

      const response = await POST('/v1/messages', request);

      expect(response.content[0].type).toBe('text');
      expect(response.content[0].text).toContain('sunny');
    });
  });

  describe('Streaming Events', () => {
    it('should emit correct SSE event sequence', async () => {
      const request = {
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1024,
        stream: true,
        messages: [{ role: 'user', content: 'Hi' }]
      };

      const events = await streamPOST('/v1/messages', request);

      // éªŒè¯äº‹ä»¶é¡ºåº
      expect(events[0].type).toBe('message_start');
      expect(events[1].type).toBe('content_block_start');
      expect(events.filter(e => e.type === 'content_block_delta').length).toBeGreaterThan(0);
      expect(events[events.length - 2].type).toBe('message_delta');
      expect(events[events.length - 1].type).toBe('message_stop');
    });
  });
});
```

### 3. Gemini æ¥å£æµ‹è¯•

```typescript
// packages/a2a-server/src/http/__tests__/geminiProxy.test.ts

describe('Gemini Proxy', () => {

  describe('Function Declarations', () => {
    it('should pass functionDeclarations to Gemini client', async () => {
      const request = {
        contents: [{ role: 'user', parts: [{ text: 'Get weather' }] }],
        generationConfig: { temperature: 0.7 },
        tools: [{
          functionDeclarations: [{
            name: 'get_weather',
            description: 'Get weather',
            parameters: {
              type: 'object',
              properties: {
                location: { type: 'string' }
              }
            }
          }]
        }]
      };

      const response = await POST('/v1beta/models/gemini-2.0-flash:generateContent', request);

      expect(response.candidates[0].content.parts).toEqual(
        expect.arrayContaining([
          expect.objectContaining({
            functionCall: expect.objectContaining({
              name: 'get_weather',
              args: expect.any(Object)
            })
          })
        ])
      );
    });
  });

  describe('System Instruction', () => {
    it('should use native systemInstruction field', async () => {
      const mockClient = {
        generateContent: jest.fn()
      };

      const request = {
        systemInstruction: 'You are a helpful assistant',
        contents: [{ role: 'user', parts: [{ text: 'Hi' }] }]
      };

      await POST('/v1beta/models/gemini-2.0-flash:generateContent', request);

      expect(mockClient.generateContent).toHaveBeenCalledWith(
        expect.anything(),
        expect.objectContaining({
          systemInstruction: 'You are a helpful assistant'
        }),
        expect.anything(),
        expect.anything()
      );
    });
  });
});
```

### 4. é›†æˆæµ‹è¯•ï¼ˆçœŸå®å®¢æˆ·ç«¯ï¼‰

```typescript
// packages/a2a-server/src/http/__tests__/integration.test.ts

import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';

describe('Real Client Integration', () => {

  it('should work with official OpenAI SDK', async () => {
    const client = new OpenAI({
      baseURL: 'http://localhost:41242/v1',
      apiKey: 'dummy'
    });

    const response = await client.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Say "test"' }]
    });

    expect(response.choices[0].message.content).toBeTruthy();
  });

  it('should work with official Anthropic SDK', async () => {
    const client = new Anthropic({
      baseURL: 'http://localhost:41242',
      apiKey: 'dummy'
    });

    const response = await client.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages: [{ role: 'user', content: 'Say "test"' }]
    });

    expect(response.content[0].type).toBe('text');
  });
});
```

### 5. æµ‹è¯•å·¥å…·å‡½æ•°

```typescript
// packages/a2a-server/src/http/__tests__/utils.ts

import { EventEmitter } from 'events';

export async function streamPOST(url: string, body: any): Promise<any[]> {
  const response = await fetch(`http://localhost:41242${url}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ ...body, stream: true })
  });

  const events: any[] = [];
  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const text = decoder.decode(value);
    const lines = text.split('\n').filter(Boolean);

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') continue;
        events.push(JSON.parse(data));
      } else if (line.startsWith('event: ')) {
        const eventType = line.slice(7);
        const dataLine = lines[lines.indexOf(line) + 1];
        if (dataLine?.startsWith('data: ')) {
          events.push({
            type: eventType,
            ...JSON.parse(dataLine.slice(6))
          });
        }
      }
    }
  }

  return events;
}
```

---

## ã€ä¿®å¤ä¼˜å…ˆçº§ã€‘

### P0 - é˜»å¡æ€§é—®é¢˜ï¼ˆå¿…é¡»ç«‹å³ä¿®å¤ï¼‰
1. **OpenAI æ¶ˆæ¯å†å²ä¿ç•™** - å½“å‰å¤šè½®å¯¹è¯å®Œå…¨ä¸å¯ç”¨
2. **OpenAI å·¥å…·è°ƒç”¨æ”¯æŒ** - ç¼ºå¤±æ ¸å¿ƒåŠŸèƒ½
3. **Claude å·¥å…·å®šä¹‰ä¼ é€’** - æ— æ³•ä¸ Gemini tools é›†æˆ
4. **Claude System Prompt** - ä½¿ç”¨åŸç”Ÿ systemInstruction
5. **Gemini å·¥å…·æ”¯æŒ** - ä¼ é€’ functionDeclarations

### P1 - é‡è¦ç¼ºå¤±ï¼ˆå°½å¿«ä¿®å¤ï¼‰
1. OpenAI stop/penalties å‚æ•°
2. Claude å·¥å…·ç»“æœå¤„ç†
3. æµå¼å“åº”é”™è¯¯å¤„ç†å¢å¼º

### P2 - å¢å¼ºåŠŸèƒ½ï¼ˆå¯åç»­ä¼˜åŒ–ï¼‰
1. Thinking/reasoning æ¨¡å¼
2. å¤šå›å¤ç”Ÿæˆ (n å‚æ•°)
3. æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜

---

## ã€æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡ã€‘

- **å•å…ƒæµ‹è¯•**: 80%+ ä»£ç è¦†ç›–
- **é›†æˆæµ‹è¯•**: 100% å…³é”®è·¯å¾„
- **å…¼å®¹æ€§æµ‹è¯•**: çœŸå® SDK å…¨åœºæ™¯

### è¿è¡Œæµ‹è¯•
```bash
# å•å…ƒæµ‹è¯•
npm test

# é›†æˆæµ‹è¯•ï¼ˆéœ€è¦å¯åŠ¨æœåŠ¡ï¼‰
npm run test:integration

# è¦†ç›–ç‡æŠ¥å‘Š
npm run test:coverage
```

---

## ã€æ€»ç»“ã€‘

**æ ¸å¿ƒé—®é¢˜**: æ•°æ®ç»“æ„è®¾è®¡è¿‡äºç®€åŒ–ï¼Œç ´åäº†å…³é”®ä¿¡æ¯

**æ ¹æœ¬åŸå› **:
- å°†å¤šè½®å¯¹è¯å‹ç¼©ä¸ºå•ä¸ªæ–‡æœ¬ â†’ ä¸¢å¤±ä¸Šä¸‹æ–‡
- æœªæ˜ å°„å·¥å…·è°ƒç”¨åè®® â†’ åŠŸèƒ½ä¸å¯ç”¨
- ç¡¬ç¼–ç æ›¿ä»£åŸç”Ÿç‰¹æ€§ â†’ æ€§èƒ½å’Œå‡†ç¡®æ€§æŸå¤±

**è§£å†³æ€è·¯**:
1. **é‡æ–°è®¾è®¡æ¶ˆæ¯æ˜ å°„** - ä¿æŒå®Œæ•´ contents æ•°ç»„
2. **å®ç°å·¥å…·åè®®è½¬æ¢** - OpenAI/Claude tools â†” Gemini functionDeclarations
3. **ä½¿ç”¨ Gemini åŸç”Ÿç‰¹æ€§** - systemInstruction, thought parts
4. **å®Œå–„æµ‹è¯•è¦†ç›–** - ç¡®ä¿åç»­æ”¹åŠ¨ä¸ç ´åå…¼å®¹æ€§

æŒ‰ç…§ Linus çš„ Good Taste åŸåˆ™ï¼š**å½“å‰å®ç°æœ‰å¤ªå¤š special casesï¼ˆæ‹¼æ¥ã€ç¡¬ç¼–ç ã€é™çº§è§£æï¼‰ï¼Œåº”è¯¥é€šè¿‡æ­£ç¡®çš„æ•°æ®ç»“æ„è®¾è®¡æ¥æ¶ˆé™¤è¿™äº›ç‰¹ä¾‹**ã€‚
